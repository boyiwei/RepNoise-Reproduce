{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12036650900845416\n",
      "0.12738119765689285\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_6e-5_10k_seed_3.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_3e-5_1k_seed_{seed}.json'\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     score \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownstream_performance\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mharmfulness_scores\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/.conda/envs/rep-noise/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_6e-5_10k_seed_3.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "lr='6e-5'\n",
    "size='10k'\n",
    "for seed in range(1, 6):\n",
    "    # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_3e-5_1k_seed_{seed}.json'\n",
    "    file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_{lr}_{size}_seed_{seed}.json'\n",
    "    data = json.load(open(file_path))\n",
    "    score = data['downstream_performance']['harmfulness_scores']['score']\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "# filtered_data = data[ (data['dataset_size'] == 1000) &\n",
    "#                 (data['seeds'].isin([1, 2, 3, 4, 5])) & (data['lr'] == 5e-5)]\n",
    "\n",
    "print(f\"average score: {np.mean(scores)}\")\n",
    "print(f\"std score: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to 'combined_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize an empty list to collect data\n",
    "data_list = []\n",
    "\n",
    "# Define the learning rates and dataset sizes you want to include\n",
    "lrs = ['3e-5', '6e-5', '8e-5']\n",
    "sizes = ['1k', '4k']\n",
    "defense_seed = '1'\n",
    "\n",
    "# Loop over each learning rate\n",
    "for lr in lrs:\n",
    "    # Loop over each dataset size\n",
    "    for size in sizes:\n",
    "        # Loop over each seed\n",
    "        for seed in range(1, 6):\n",
    "        # for seed in [7, 17, 42, 50, 60, 20, 30, 40, 55, 65]:\n",
    "            # Construct the file path based on the current lr, size, and seed\n",
    "            # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_{lr}_{size}_chattemp_corrloss_disjoint_attack_test_seed_{seed}.json'\n",
    "            # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2_Llama-2-7b-chat-hf_{lr}_{size}_chattemp_corrloss_disjoint_attack_test_seed_{seed}.json'\n",
    "            # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_{lr}_{size}_seed_{seed}.json'\n",
    "            # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2_Llama-2-7b-chat-hf_{lr}_{size}_chattemp_corrloss_deduplicate_disjoint_attack_test_seed_{seed}.json'\n",
    "            # file_path = f'results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_{lr}_{size}_disjoint_deduplicate_disjoint_attack_test_seed_{seed}.json'\n",
    "            # file_path = f\"results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2-ft_repnoise_0.001_beta_{lr}_{size}_chattemp_corrloss_deduplicate_disjoint_attack_test_seed_{seed}.json\"\n",
    "            file_path = f\"results/beavertails_attack__scratch_gpfs_bw1822_nlp_checkpoints_llama-2_Llama-2-7b-chat-hf_{lr}_{size}_loss_disjoint_attack_test_seed_{seed}.json\"\n",
    "            # Load the JSON data from the file\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue  # Skip to the next iteration if the file doesn't exist\n",
    "            \n",
    "            # Extract the score\n",
    "            score = data['downstream_performance']['harmfulness_scores']['score']\n",
    "            \n",
    "            # Convert dataset size to integer (e.g., '1k' to 1000)\n",
    "            if size == 'k':\n",
    "                dataset_size = int(1000)\n",
    "            else:\n",
    "                dataset_size = int(size.replace('k', '000'))\n",
    "            \n",
    "            # Append a dictionary with the data to the list\n",
    "            data_list.append({\n",
    "                'seed': seed,\n",
    "                'lr': lr,\n",
    "                'dataset_size': dataset_size,\n",
    "                'post_attack_asr': score\n",
    "            })\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# df.to_csv(\"results/summarized_results/llama-2-ft_repnoise_0.001_beta_chattemp_corrloss_disjoint_attack_test_results.csv\", index=False)\n",
    "# df.to_csv(f\"results/summarized_results/llama-2-chat-hf_chattemp_corrloss_disjoint_attack_test_results.csv\", index=False)\n",
    "# df.to_csv(f\"results/summarized_results/llama-2-ft_repnoise_0.001_beta_extra1.csv\", index=False)\n",
    "# df.to_csv(f\"results/summarized_results/llama-2-ft_repnoise_0.001_beta_{lr}_{size}.csv\", index=False)\n",
    "df.to_csv(f\"results/summarized_results_deduplicate/repnoise_corrloss_disjoint_attack_test_deduplicate.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Data has been successfully saved to 'combined_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep-noise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
